|                            网络                             | miou   |
| :---------------------------------------------------------: | ------ |
|                   baseline hrnet-contrast                   | 71.87% |
|                         通道数减半                          | -0.72% |
|                      将relu替换成frelu                      | +2.71% |
|                  将decoder替换成原始的ocr                   | +0.35% |
|          加入segfix后处理（与加载预训练模型81.4%）          | -5.4%  |
|             将backbone中的卷积替换为repvggblock             | +3.68% |
|                        加入psa注意力                        | -2.51% |
| 加入将coordinateattention和strippooling顺序链接组成的注意力 | -0.48% |
|     将ocrdecoder进行修改加入边缘辅助头(与原始的ocr相比)     | +1.01% |
|                  将部分卷积改成非重叠卷积                   | -8.07% |
|       将bottleneck中的第二个conv替换成involution算子        | +1.03% |
|                 将eca注意力加进bottleneck层                 | +1.09% |
|                        将bn替换为gn                         | -8.5%  |

通道数减半是为了看通道的冗余程度，然后精度减少也在可接受的范围，本来这个实验是为了之后的剪枝，想了想还是放弃了。

将relu替换成frelu的话是因为relu本身x小于0的那一段，就有各种问题和改进，然后frelu的话是把它拓展到了2d，相当于增加了一个空间相关性。

将原有的baseline decoder换成ocr的话是因为paddle本身没有ocrnet的baseline，然后当时也是打算改一下ocr注意力就先跑了一下，事实也确定是上下文关系比像素关系的效果更好。

将卷积替换成repvggblock是因为它的重参数化非常新颖，而且重整以后不会造成计算量的增加。

加入psa注意力算一个尝试吧，毕竟注意力谁都想加一下，然后psa其实和dual attention很像，双注意力连接，结果寄了，我当时的想法是金字塔的原因，毕竟下采样会损失信息，或者直接因为参数量过大过拟合了。

于是自己借鉴cbam的思想，找了一个通道注意力和空间注意力顺序链接，就是上面说的那两玩意，结果有点小寄，然后我觉得hrnet可能并不需要空间和通道注意力，因为它自己本身就存在一个高分辨率的分支，然后多分支不同分辨率不同通道融合，我觉得它可能已经把空间和通道注意力用完了。

之后就开始改ocrdecoder了，因为毕竟上下文还是缺了点边缘注意的，因为上下文说到底就是高层语义信息，而高层语义信息会损失一些边缘这种底层语义信息，当时看见segfix这种边缘细化后处理就用了，结果很寄，然后就去找边缘细化方面的，看到stdcseg有一个detail head模块，我就把他融合到了ocr中，将边缘信息加进了ocr的pixels，也就是backbone得到的深层特征图。

将部分卷积改成非重叠卷积是我天天看见transformer，然后看到了一个convnext，它的实验中借鉴了transformer中的patch思想，当然还有一个convmixer，patches is all you need，然后我因为不想爆显存，就没完全借鉴patchs，只是将部分卷积换成非重叠卷积大概试试，因为如果卷积不重叠的话，不就相当于将图片分块吗。虽然想过会掉。没想到这么夸张，不过也是，原来两倍下采样，换成非重叠以后变成三倍下采样了，倒也合理。

而involution的话，主要是我想优化一下bottleneck，然后往那方面了解了一下，在加上这个看起来挺新颖的，就用了

而groupnormalization的话是因为我的batchsize比较小，只有4，然后就看见groupnormlization可以在batchsize比较小的情况下取得跟大batchsize相同的效果，而且不会那么吃显存，就尝试了，结果寄了，当时看见将通道分组就让我想到了组卷积，而这样会破坏通道间的交互关系，我分了16组，结果一个组大概也就2,3个通道的样子，然后寄了，好像还能像shufflenet一样配一个channelshuffle什么的，但那样意义就不大了，不如直接用bn

eca注意力的话，是简化版的通道注意力，因为我不甘心注意力一个都不成功，所以进行了最后的尝试，我觉得之前的注意力可能由于太臃肿没能得到一个很好的效果，所以加了一个最简单的eca注意力在bottleneck的第二个卷积后面，在耗费比involution少的计算量下取得了相同程度的提升。
